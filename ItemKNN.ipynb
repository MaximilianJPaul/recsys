{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c148bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9079895",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4345c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, sep='\\t'):\n",
    "    return pd.read_csv(f'./lfm-challenge-data/{file}', delimiter=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e376cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = load_data('lfm-challenge.user')\n",
    "items = load_data('lfm-challenge.item')\n",
    "inter_train = load_data('lfm-challenge.inter_train')\n",
    "inter_test = load_data('lfm-challenge.inter_test')\n",
    "test_users = pd.read_csv(f'./lfm-challenge-data/test_indices.txt')['users'].values\n",
    "\n",
    "n_users = users['user_id'].values.size\n",
    "n_items = items.index.values.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b234591d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4251, 9092, 6483, 4517, 4353, 7505, 1504, 3152, 1606, 6897, 1771,\n",
       "       4815, 4173, 7909, 3592, 6689, 8063, 1954, 8530, 9346, 2202, 8896,\n",
       "       8598, 1247, 1572, 8070, 7687, 1849, 7330, 1367, 2340, 5343, 6779,\n",
       "       5069, 5256, 2810, 8733, 7546, 6189, 7438,  900, 8155, 5282, 3762,\n",
       "       4289, 4618, 6097,  657, 7312, 2211, 6274, 8691, 6594, 3554, 4318,\n",
       "       4493, 8899, 4947, 7072, 7183, 3882, 3577, 7421,  744, 6172, 6617,\n",
       "       7970,  836, 2684,  956, 9359, 7305,  427, 8231, 6749, 4235, 8257,\n",
       "       1895, 3522,  620,  919, 4820, 9368, 4227, 3030, 7789, 6084, 6538,\n",
       "       4554, 7514, 3982, 3426, 7198, 1272, 6637, 3195,  870, 5462, 4347,\n",
       "       8060])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c9b7a",
   "metadata": {},
   "source": [
    "# Interaction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "503ee769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_matrix(users, items, inter, threshold=1, binary=False):\n",
    "    interaction_matrix = np.zeros((n_users, n_items), dtype=np.int8)\n",
    "    \n",
    "    for user in range(n_users):\n",
    "        interacted_items = inter.loc[inter['user_id'] == user, 'item_id'].values\n",
    "        rate_of_items = inter.loc[inter['user_id'] == user, 'listening_events'].values\n",
    "        \n",
    "        for item in range(interacted_items.size):\n",
    "            rating = rate_of_items[item]\n",
    "            if binary:\n",
    "                rating = 0 if rating < threshold else 1\n",
    "            \n",
    "            interaction_matrix[user, interacted_items[item]] = rating\n",
    "    \n",
    "    return interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1142ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix = create_interaction_matrix(users, items, inter_train, binary=True)\n",
    "test_interaction_matrix = create_interaction_matrix(users, items, inter_test, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9debf",
   "metadata": {},
   "source": [
    "# ItemKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47d32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1e5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0cccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ad0655",
   "metadata": {},
   "source": [
    "# Get Item Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c77c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_score(a: np.array, b: np.array) -> float:\n",
    "    \"\"\"\n",
    "    a, b: - vectors of the same length corresponding to the two items\n",
    "\n",
    "    returns: float - jaccard similarity score for a and b\n",
    "    \"\"\"\n",
    "    score = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    intersection = np.logical_and(a, b).sum()\n",
    "    union = np.logical_or(a, b).sum()\n",
    "    score = intersection / union\n",
    "\n",
    "    return float(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf06f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sim_scores(similarity_measure: Callable[[int, int], float],\n",
    "                         inter: np.array,\n",
    "                         target_vec: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    similarity_measure: Callable - function that measures similarity, use your jaccard_score function from above\n",
    "    inter: np.array - interaction matrix - calculate similarity between each item and the target item (see below)\n",
    "    target_vec: np.array - target item vector\n",
    "    \n",
    "    returns: np.array - similarities between every item from <inter> and <target_vec> in the respective order\n",
    "    \"\"\"\n",
    "\n",
    "    item_similarities = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "    def func(col, target):\n",
    "        return similarity_measure(col, target)\n",
    "    \n",
    "    item_similarities = np.apply_along_axis(func, axis=0, arr=inter, target=target_vec)\n",
    "\n",
    "    return np.array(item_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "878b2933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.02702703, 0.        , ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we pass your jaccard_score function as \"similarity_measure\" parameter\n",
    "item_sims = calculate_sim_scores(similarity_measure=jaccard_score, inter=interaction_matrix, target_vec=interaction_matrix[:, 0])\n",
    "item_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873467ba",
   "metadata": {},
   "source": [
    "# Estimate User Item Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9810316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_score(sim_scores_calculator: Callable[[Callable, np.array, np.array], np.array],\n",
    "                        inter: np.array,\n",
    "                        target_user: int,\n",
    "                        target_item: int,\n",
    "                        n: int = 2) -> float:\n",
    "    \"\"\"\n",
    "    sim_scores_calculator: Callable - function that calculates similarities, using calculate_sim_scores\n",
    "                                      from above, already defined in the next cell\n",
    "    inter: np.array - interaction matrix\n",
    "    target_user: target user id\n",
    "    target_item: int - target item id\n",
    "    n: int - n closest neighbors to consider for the score prediction\n",
    "    \n",
    "    returns: float - mean of similarity scores = user-item 'fitness' score\n",
    "    \"\"\"\n",
    "\n",
    "    item_similarities_mean = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    consumed_items = np.where(inter[target_user] == 1)[0]\n",
    "    new_inter = np.delete(inter[:, consumed_items], target_user, axis=0)\n",
    "    target_vector = np.delete(inter, target_user, axis=0)[:, target_item]\n",
    "    \n",
    "    item_similarities_mean = np.mean(np.sort(sim_scores_calculator(new_inter, target_vector))[::-1][:n])\n",
    "    \n",
    "\n",
    "    return item_similarities_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce100962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026919362626750362\n"
     ]
    }
   ],
   "source": [
    "# you need to pass a \"sim_scores_calculator\" function into the \"get_user_item_score\" function,\n",
    "# but \"calculate_sim_scores\" also takes a similarity measure function as parameter.\n",
    "# The similarity measure is not necessarily present inside the \"get_user_item_score\" function\n",
    "# Ideally you want to provide the similarity measure together with the \"calculate_sim_scores\" function\n",
    "\n",
    "# The following line of code is one possible way to \"bind\" parameters to a function: you can now use the \"sim_score_calc\" function as parameter,\n",
    "# which will always use your \"jaccard_score\" function as first parameter for \"calculate_sim_scores\" and passes through the other parameters\n",
    "# This procedure is a way to keep your functions generic, you can now simply change your similarity measure via the\n",
    "# function calls without needing to change the function bodies themselves\n",
    "#\n",
    "# Another advantage for you is that when we test your solution, we are going to pass our own implementations into your functions\n",
    "# That means if you made a mistake in Task 1, you will still be able to get full points for consequent tasks if you did everything else correctly\n",
    "# So make sure that your functions work independently from your other implemented functions by using the code we provide in this cell\n",
    "\n",
    "def sim_score_calc(inter, target_vec): return calculate_sim_scores(jaccard_score, inter, target_vec)\n",
    "\n",
    "\n",
    "# TODO: Fill in the missing parameters\n",
    "item_sim = get_user_item_score(sim_score_calc, interaction_matrix, 0, 1, 10)\n",
    "\n",
    "print(item_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55cb6c",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b14351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recTopK(user_item_scorer: Callable[[Callable, np.array, int, int], float],\n",
    "            inter_matr: np.array,\n",
    "            user: int,\n",
    "            top_k: int,\n",
    "            n: int) -> (np.array, np.array):\n",
    "    '''\n",
    "    user_item_scorer: Callable - wrapper function that calculates user-item score, using get_user_item_score function\n",
    "                                 from above, already defined in the next cell\n",
    "    inter_matr: np.array - interaction matrix\n",
    "    user: int -  user_id\n",
    "    top_k: int - expected length of the resulting list\n",
    "    n: int - number of neighbors to consider\n",
    "    \n",
    "    returns - array of recommendations (sorted in the order of descending scores) & array of corresponding scores\n",
    "    '''\n",
    "\n",
    "    top_rec = None\n",
    "    scores = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION.\n",
    "    unseen_items = np.where(inter_matr[user] == 0)[0]\n",
    "    scores = []\n",
    "    \n",
    "    for item in unseen_items:\n",
    "        score = user_item_scorer(inter_matr, user, item, n)\n",
    "        scores.append(score)\n",
    "      \n",
    "    top_rec = unseen_items[np.argsort(scores)[::-1][:top_k]]\n",
    "    scores = np.array(np.sort(scores)[::-1][:top_k])\n",
    "\n",
    "    return np.array(top_rec), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4759475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Task 2 for an explanation of the following function definition\n",
    "def user_item_scorer(inter, target_user, target_item, n): return get_user_item_score(sim_score_calc, inter,\n",
    "                                                                                     target_user, target_item, n)\n",
    "\n",
    "\n",
    "# TODO: Fill in the missing parameters\n",
    "rec_item_cf, scores_item_cf = recTopK(user_item_scorer, interaction_matrix, 0, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a733bda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\n",
      "1/100\n",
      "2/100\n",
      "3/100\n",
      "4/100\n",
      "5/100\n",
      "6/100\n",
      "7/100\n",
      "8/100\n",
      "9/100\n",
      "10/100\n",
      "11/100\n",
      "12/100\n",
      "13/100\n",
      "14/100\n",
      "15/100\n",
      "16/100\n",
      "17/100\n",
      "18/100\n",
      "19/100\n",
      "20/100\n",
      "21/100\n",
      "22/100\n",
      "23/100\n",
      "24/100\n",
      "25/100\n",
      "26/100\n",
      "27/100\n",
      "28/100\n",
      "29/100\n",
      "30/100\n",
      "31/100\n",
      "32/100\n",
      "33/100\n",
      "34/100\n",
      "35/100\n",
      "36/100\n",
      "37/100\n",
      "38/100\n",
      "39/100\n",
      "40/100\n",
      "41/100\n",
      "42/100\n",
      "43/100\n",
      "44/100\n",
      "45/100\n",
      "46/100\n",
      "47/100\n",
      "48/100\n",
      "49/100\n",
      "50/100\n",
      "51/100\n",
      "52/100\n",
      "53/100\n",
      "54/100\n",
      "55/100\n",
      "56/100\n",
      "57/100\n",
      "58/100\n",
      "59/100\n",
      "60/100\n",
      "61/100\n",
      "62/100\n",
      "63/100\n",
      "64/100\n",
      "65/100\n",
      "66/100\n",
      "67/100\n",
      "68/100\n",
      "69/100\n",
      "70/100\n",
      "71/100\n",
      "72/100\n",
      "73/100\n",
      "74/100\n",
      "75/100\n",
      "76/100\n",
      "77/100\n",
      "78/100\n",
      "79/100\n",
      "80/100\n",
      "81/100\n",
      "82/100\n",
      "83/100\n",
      "84/100\n",
      "85/100\n",
      "86/100\n",
      "87/100\n",
      "88/100\n",
      "89/100\n",
      "90/100\n",
      "91/100\n",
      "92/100\n",
      "93/100\n",
      "94/100\n",
      "95/100\n",
      "96/100\n",
      "97/100\n",
      "98/100\n",
      "99/100\n"
     ]
    }
   ],
   "source": [
    "def recommend():\n",
    "    pred = []\n",
    "    \n",
    "    for i, user_id in enumerate(test_users):\n",
    "        print(f\"{i}/{len(test_users)}\")\n",
    "        rec_item_cf, scores_item_cf = recTopK(user_item_scorer, interaction_matrix, user_id, 10, 4)\n",
    "        pred.append(rec_item_cf)\n",
    "\n",
    "    return np.array(pred)\n",
    "        \n",
    "predictions = recommend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1781fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('my_array.npy', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9800f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndcg_score(predictions: np.ndarray, test_interaction_matrix: np.ndarray, topK=10) -> float:\n",
    "    \"\"\"\n",
    "    predictions - np.ndarray - predictions of the recommendation algorithm for each user.\n",
    "    test_interaction_matrix - np.ndarray - test interaction matrix for each user.\n",
    "    topK - int - topK recommendations should be evaluated.\n",
    "    \n",
    "    returns - average ndcg score over all users.\n",
    "    \"\"\"\n",
    "    n_users = predictions.shape[0]\n",
    "    discounts = np.log2(np.arange(2, topK+2)) # discounts for positions 1 to topK (0-indexed)\n",
    "    ndcg_scores = np.zeros(n_users)\n",
    "\n",
    "    for user in range(n_users):\n",
    "        top_items = predictions[user]\n",
    "        relevant_items = test_interaction_matrix[user].nonzero()[0]\n",
    "        if len(relevant_items) == 0:\n",
    "            continue\n",
    "\n",
    "        # calculate DCG\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(top_items):\n",
    "            if item in relevant_items:\n",
    "                dcg += 1 / discounts[i]\n",
    "\n",
    "        # calculate IDCG\n",
    "        n_relevant = min(topK, len(relevant_items))\n",
    "        idcg = np.sum(1 / discounts[:n_relevant]) # it is 1 for each relevant item in ideal case\n",
    "        \n",
    "        ndcg_scores[user] = dcg / idcg\n",
    "\n",
    "    return np.mean(ndcg_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21f89360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002200917662980802"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = get_ndcg_score(predictions, test_interaction_matrix)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087365a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7683ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6540a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations with Item CF:  [   2  184 1626 2061 2249 2242 1304 2070    5  177]\n",
      "With Scores:  [0.06069266 0.05301205 0.05008259 0.04966753 0.04560784 0.04372179\n",
      " 0.04112483 0.0405     0.03960521 0.03960249]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Recommendations with Item CF: \", rec_item_cf)\n",
    "print(\"With Scores: \", scores_item_cf)\n",
    "print(\"-\" * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca6a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
